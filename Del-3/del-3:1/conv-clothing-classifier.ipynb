{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('../del-3:2/Large-mixed-clothing-dataset/myntradataset/styles.csv', on_bad_lines='skip', header=None, names=[\"id\", \"gender\", \"masterCategory\", \"subCategory\", \"articleType\", \"baseColor\", \"season\", \"year\", \"usage\", \"productDisplayName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "['articleType', 'Shirts', 'Jeans', 'Watches', 'Track Pants', 'Tshirts', 'Socks', 'Casual Shoes', 'Belts', 'Flip Flops', 'Handbags', 'Tops', 'Bra', 'Sandals', 'Shoe Accessories', 'Sweatshirts', 'Deodorant', 'Formal Shoes', 'Bracelet', 'Lipstick', 'Flats', 'Kurtas', 'Waistcoat', 'Sports Shoes', 'Shorts', 'Briefs', 'Sarees', 'Perfume and Body Mist', 'Heels', 'Sunglasses', 'Innerwear Vests', 'Pendant', 'Nail Polish', 'Laptop Bag', 'Scarves', 'Rain Jacket', 'Dresses', 'Night suits', 'Skirts', 'Wallets', 'Blazers', 'Ring', 'Kurta Sets', 'Clutches', 'Shrug', 'Backpacks', 'Caps', 'Trousers', 'Earrings', 'Camisoles', 'Boxers', 'Jewellery Set', 'Dupatta', 'Capris', 'Lip Gloss', 'Bath Robe', 'Mufflers', 'Tunics', 'Jackets', 'Trunk', 'Lounge Pants', 'Face Wash and Cleanser', 'Necklace and Chains', 'Duffel Bag', 'Sports Sandals', 'Foundation and Primer', 'Sweaters', 'Free Gifts', 'Trolley Bag', 'Tracksuits', 'Swimwear', 'Shoe Laces', 'Fragrance Gift Set', 'Bangle', 'Nightdress', 'Ties', 'Baby Dolls', 'Leggings', 'Highlighter and Blush', 'Travel Accessory', 'Kurtis', 'Mobile Pouch', 'Messenger Bag', 'Lip Care', 'Face Moisturisers', 'Compact', 'Eye Cream', 'Accessory Gift Set', 'Beauty Accessory', 'Jumpsuit', 'Kajal and Eyeliner', 'Water Bottle', 'Suspenders', 'Lip Liner', 'Robe', 'Salwar and Dupatta', 'Patiala', 'Stockings', 'Eyeshadow', 'Headband', 'Tights', 'Nail Essentials', 'Churidar', 'Lounge Tshirts', 'Face Scrub and Exfoliator', 'Lounge Shorts', 'Gloves', 'Mask and Peel', 'Wristbands', 'Tablet Sleeve', 'Ties and Cufflinks', 'Footballs', 'Stoles', 'Shapewear', 'Nehru Jackets', 'Salwar', 'Cufflinks', 'Jeggings', 'Hair Colour', 'Concealer', 'Rompers', 'Body Lotion', 'Sunscreen', 'Booties', 'Waist Pouch', 'Hair Accessory', 'Rucksacks', 'Basketballs', 'Lehenga Choli', 'Clothing Set', 'Mascara', 'Toner', 'Cushion Covers', 'Key chain', 'Makeup Remover', 'Lip Plumper', 'Umbrellas', 'Face Serum and Gel', 'Hat', 'Mens Grooming Kit', 'Rain Trousers', 'Body Wash and Scrub', 'Suits', 'Ipad']\n"
     ]
    }
   ],
   "source": [
    "unique_classes = dataframe['articleType'].unique()\n",
    "class_list = list(unique_classes)\n",
    "print(len(class_list))\n",
    "print(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "['articleType', 'Shirts', 'Jeans', 'Watches', 'Track Pants', 'Tshirts', 'Socks', 'Casual Shoes', 'Belts', 'Flip Flops', 'Handbags', 'Tops', 'Bra', 'Sandals', 'Sweatshirts', 'Formal Shoes', 'Bracelet', 'Shorts', 'Heels', 'Sunglasses', 'Innerwear Vests', 'Laptop Bag', 'Scarves', 'Rain Jacket', 'Dresses', 'Night suits', 'Skirts', 'Wallets', 'Blazers', 'Ring', 'Backpacks', 'Caps', 'Trousers', 'Earrings', 'Boxers', 'Bath Robe', 'Tunics', 'Jackets', 'Trunk', 'Necklace and Chains', 'Sweaters', 'Trolley Bag', 'Tracksuits', 'Swimwear', 'Nightdress', 'Ties', 'Leggings', 'Beauty Accessory', 'Jumpsuit', 'Water Bottle', 'Suspenders', 'Robe', 'Stockings', 'Headband', 'Tights', 'Gloves', 'Wristbands', 'Booties', 'Waist Pouch', 'Hair Accessory', 'Umbrellas', 'Hat', 'Rain Trousers', 'Suits']\n"
     ]
    }
   ],
   "source": [
    "items_to_drop = [\n",
    "    \"Ipad\", \"Body Wash and Scrub\", \"Mens Grooming Kit\", \"Face Serum and Gel\", \"Lip Plumper\", \"Makeup Remover\", \"Key chain\",\n",
    "    \"Rucksacks\", \"Basketballs\", \"Lehenga Choli\", \"Clothing Set\", \"Mascara\", \"Toner\", \"Cushion Covers\", \"Tablet Sleeve\",\n",
    "    \"Ties and Cufflinks\", \"Footballs\", \"Stoles\", \"Shapewear\", \"Nehru Jackets\", \"Salwar\", \"Cufflinks\", \"Jeggings\", \n",
    "    \"Hair Colour\", \"Concealer\", \"Rompers\", \"Body Lotion\", \"Sunscreen\", \"Mask and Peel\", \"Face Scrub and Exfoliator\",\n",
    "    \"Lounge Shorts\", \"Nail Essentials\", \"Churidar\", \"Lounge Tshirts\", \"Eyeshadow\", \"Salwar and Dupatta\", \"Patiala\",\n",
    "    \"Lip Liner\", \"Kajal and Eyeliner\", \"Highlighter and Blush\", \"Travel Accessory\", \"Kurtis\", \"Mobile Pouch\", \n",
    "    \"Messenger Bag\", \"Lip Care\", \"Face Moisturisers\", \"Compact\", \"Eye Cream\", \"Accessory Gift Set\", \"Baby Dolls\",\n",
    "    \"Shoe Laces\", \"Fragrance Gift Set\", \"Bangle\", \"Free Gifts\", \"Duffel Bag\", \"Sports Sandals\", \n",
    "    \"Foundation and Primer\", \"Lounge Pants\", \"Face Wash and Cleanser\", \"Mufflers\", \"Jewellery Set\", \"Dupatta\", \n",
    "    \"Capris\", \"Lip Gloss\", \"Camisoles\", \"Kurta Sets\", \"Clutches\", \"Shrug\", \"Pendant\", \"Nail Polish\", \"Briefs\", \n",
    "    \"Sarees\", \"Perfume and Body Mist\", \"Lipstick\", \"Flats\", \"Kurtas\", \"Waistcoat\", \"Sports Shoes\", \"Deodorant\", \n",
    "    \"Shoe Accessories\"\n",
    "]\n",
    "\n",
    "filtered_dataframe = dataframe[~dataframe['articleType'].isin(items_to_drop)]\n",
    "\n",
    "unique_classes = filtered_dataframe['articleType'].unique()\n",
    "class_list = list(unique_classes)\n",
    "print(len(class_list))\n",
    "print(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "['articleType', 'Shirts', 'Pants', 'Watches', 'T-shirts', 'Accessories', 'Shoes', 'Belts', 'Slippers', 'Bags', 'Sweaters', 'Shorts', 'Outerwear', 'Dresses', 'Robe', 'Skirts', 'Hats', 'One-piece', 'Swimwear', 'Ties', 'Headband', 'Gloves', 'Umbrellas', 'Suits']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hl/96z92_2n0ln6nc1ssp8d7h040000gn/T/ipykernel_21391/154197861.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_dataframe['general_articleType'] = filtered_dataframe['articleType'].map(label_mapping)\n",
      "/var/folders/hl/96z92_2n0ln6nc1ssp8d7h040000gn/T/ipykernel_21391/154197861.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  filtered_dataframe['general_articleType'].fillna(filtered_dataframe['articleType'], inplace=True)\n",
      "/var/folders/hl/96z92_2n0ln6nc1ssp8d7h040000gn/T/ipykernel_21391/154197861.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_dataframe['general_articleType'].fillna(filtered_dataframe['articleType'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {\n",
    "    'Shirts': 'Shirts', 'Jeans': 'Pants', 'Watches': 'Watches', 'Track Pants': 'Pants',\n",
    "    'Tshirts': 'T-shirts', 'Socks': 'Accessories', 'Casual Shoes': 'Shoes', 'Belts': 'Belts',\n",
    "    'Flip Flops': 'Slippers', 'Handbags': 'Bags', 'Tops': 'Shirts', 'Bra': 'Accessories',\n",
    "    'Sandals': 'Shoes', 'Sweatshirts': 'Sweaters', 'Formal Shoes': 'Shoes', 'Bracelet': 'Accessories',\n",
    "    'Waistcoat': 'Outerwear', 'Sports Shoes': 'Shoes', 'Shorts': 'Shorts', 'Heels': 'Shoes',\n",
    "    'Sunglasses': 'Accessories', 'Innerwear Vests': 'Accessories', 'Laptop Bag': 'Bags',\n",
    "    'Scarves': 'Accessories', 'Rain Jacket': 'Outerwear', 'Dresses': 'Dresses', 'Night suits': 'Robe',\n",
    "    'Skirts': 'Skirts', 'Wallets': 'Accessories', 'Blazers': 'Outerwear', 'Ring': 'Accessories',\n",
    "    'Backpacks': 'Bags', 'Caps': 'Hats', 'Trousers': 'Pants', 'Earrings': 'Accessories',\n",
    "    'Boxers': 'Accessories', 'Bath Robe': 'Robe', 'Tunics': 'Dresses', 'Jackets': 'Outerwear',\n",
    "    'Trunk': 'Bags', 'Lounge Pants': 'Pants', 'Necklace and Chains': 'Accessories', 'Sports Sandals': 'Shoes',\n",
    "    'Sweaters': 'Sweaters', 'Trolley Bag': 'Bags', 'Tracksuits': 'One-piece', 'Swimwear': 'Swimwear',\n",
    "    'Nightdress': 'Robe', 'Ties': 'Ties', 'Leggings': 'Pants', 'Beauty Accessory': 'Accessories',\n",
    "    'Jumpsuit': 'One-piece', 'Water Bottle': 'Accessories', 'Suspenders': 'Accessories',\n",
    "    'Robe': 'Robe', 'Stockings': 'Pants', 'Headband': 'Headband', 'Tights': 'Pants',\n",
    "    'Gloves': 'Gloves', 'Wristbands': 'Accessories', 'Nehru Jackets': 'Outerwear', \n",
    "    'Jeggings': 'Pants', 'Booties': 'Shoes', 'Waist Pouch': 'Accessories', 'Hair Accessory': 'Accessories',\n",
    "    'Umbrellas': 'Umbrellas', 'Hat': 'Hats', 'Rain Trousers': 'Pants', 'Suits': 'Suits'\n",
    "}\n",
    "\n",
    "# Apply this dictionary to map your dataset's 'articleType' to a new 'general_articleType'\n",
    "filtered_dataframe['general_articleType'] = filtered_dataframe['articleType'].map(label_mapping)\n",
    "\n",
    "# Handling any potential unmapped items\n",
    "filtered_dataframe['general_articleType'].fillna(filtered_dataframe['articleType'], inplace=True)\n",
    "\n",
    "unique_classes = filtered_dataframe['general_articleType'].unique()\n",
    "class_list = list(unique_classes)\n",
    "print(len(class_list))\n",
    "print(class_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34034\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "scheduler_step_size = 7\n",
    "scheduler_gamma = 0.1\n",
    "num_epochs = 20\n",
    "degrees = 45\n",
    "translate = (0.1, 0.3)\n",
    "scale = (0.8, 1.2)\n",
    "saturation = 0.5\n",
    "num_workers = 2\n",
    "########\n",
    "\n",
    "\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Dataframe containing image info.\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.label_map = {label: idx for idx, label in enumerate(\n",
    "            dataframe['general_articleType'].unique())}\n",
    "\n",
    "        self.dataframe = dataframe.copy()\n",
    "        self.dataframe['exists'] = self.dataframe['id'].apply(\n",
    "            lambda x: os.path.exists(os.path.join(root_dir, f\"{x}.jpg\")))\n",
    "        self.dataframe = self.dataframe[self.dataframe['exists']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def max_label(self):\n",
    "        \"\"\"Returns the maximum label integer from the dataframe.\"\"\"\n",
    "        return self.dataframe['general_articleType'].map(self.label_map).max()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['id'] + \".jpg\"\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        label_str = self.dataframe.iloc[idx]['general_articleType']\n",
    "        label_int = self.label_map[label_str]\n",
    "\n",
    "        label = torch.tensor(label_int, dtype=torch.long)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "dataset = ClothingDataset(dataframe=filtered_dataframe,\n",
    "                          root_dir='../del-3:2/Large-mixed-clothing-dataset/myntradataset/images')\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)  # 80% for training\n",
    "test_size = total_size - train_size  # 20% for testing\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=degrees, translate=translate, scale=scale),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(saturation=saturation),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "def train_collate_fn(batch):\n",
    "    transformed_batch = [(train_transform(x), y) for x, y in batch]\n",
    "    return torch.utils.data.dataloader.default_collate(transformed_batch)\n",
    "\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    transformed_batch = [(test_transform(x), y) for x, y in batch]\n",
    "    return torch.utils.data.dataloader.default_collate(transformed_batch)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=True, collate_fn=train_collate_fn)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=num_workers, pin_memory=True, collate_fn=test_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "['Shirts' 'Pants' 'Watches' 'T-shirts' 'Accessories' 'Shoes' 'Belts'\n",
      " 'Slippers' 'Bags' 'Sweaters' 'Shorts' 'Outerwear' 'Dresses' 'Robe'\n",
      " 'Skirts' 'Hats' 'One-piece' 'Swimwear' 'Ties' 'Headband' 'Gloves'\n",
      " 'Umbrellas']\n"
     ]
    }
   ],
   "source": [
    "class_list = dataset.dataframe['general_articleType'].unique()\n",
    "print(len(class_list))\n",
    "print(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'ClothingDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m             images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Skolprojekt/mnist/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Skolprojekt/mnist/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Skolprojekt/mnist/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(CNN,self).__init__()\n",
    "      self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1)\n",
    "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "      self.bn1 = nn.BatchNorm2d(64)\n",
    "      self.dropout1 = nn.Dropout2d(0.25)\n",
    "      self.dropout2 = nn.Dropout2d(0.5)\n",
    "      self.fc1 = nn.Linear(1600, 128)\n",
    "      self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.conv1(x)\n",
    "      x = nn.functional.relu(x)\n",
    "      x = nn.functional.max_pool2d(x, 2)\n",
    "      x = self.dropout1(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.bn1(x)\n",
    "      x = nn.functional.relu(x)\n",
    "      x = nn.functional.max_pool2d(x, 2)\n",
    "      x = self.dropout1(x)\n",
    "      x = torch.flatten(x, 1)\n",
    "      x = self.fc1(x)\n",
    "      x = nn.functional.relu(x)\n",
    "      x = self.dropout2(x)\n",
    "      x = self.fc2(x)\n",
    "      output = nn.functional.log_softmax(x, dim=1)\n",
    "      return output\n",
    "    \n",
    "loss_history = []\n",
    "\n",
    "model = CNN()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "step_lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        try:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 694 == 0:\n",
    "                print(\n",
    "                    f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item()}')\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {e}, skipping...\")\n",
    "            continue\n",
    "\n",
    "    step_lr_scheduler.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    num_classes = len(dataset.dataframe)  # Assuming this attribute exists and is set correctly\n",
    "    n_class_correct = [0 for _ in range(num_classes)]\n",
    "    n_class_samples = [0 for _ in range(num_classes)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "acc = 100.0 * n_correct / n_samples\n",
    "print(f'Accuracy of the network on the test images: {acc} %')\n",
    "\n",
    "for i in range(num_classes):\n",
    "    if n_class_samples[i] != 0:\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {class_list[i]}: {acc:.2f} %')\n",
    "    else:\n",
    "        print(f'Accuracy of {class_list[i]}: N/A (no samples)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
